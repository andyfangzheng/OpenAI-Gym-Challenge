{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Case Study 3 : Data Science in OpenAI Games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Required Readings:** \n",
    "* [Deep Reinforcement Learning](http://karpathy.github.io/2016/05/31/rl/) \n",
    "* [OpenAI](https://gym.openai.com/)\n",
    "* [TED Talks](https://www.ted.com/talks) for examples of 10 minutes talks.\n",
    "\n",
    "** NOTE **\n",
    "* In this case study, you will need to use Mac OS or Linux system. In windows computers, you could use virtualbox to create a virtual machine and install an Ubuntu OS in the virtual machine.\n",
    "* Please don't forget to save the notebook frequently when working in Jupyter Notebook, otherwise the changes you made can be lost.\n",
    "\n",
    "*----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Problem: pick a data science problem that you plan to solve using OpenAI Games\n",
    "* The problem should be important and interesting, which has a potential impact in some area.\n",
    "* The problem should be solvable using the openAI game system and data science solutions.\n",
    "\n",
    "Please briefly describe in the following cell: what problem are you trying to solve? why this problem is important and interesting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Group 6 \n",
    "# Huanhan Liu\n",
    "# Qian Xu \n",
    "# Aashish Bagul\n",
    "# Fangzheng Sun\n",
    "\n",
    "# In this case, we want to use Atari games to see if deep reinforcement learning can let computer automatically play those games and try to\n",
    "# get more scores episode by episode.\n",
    "\n",
    "# If we can approve that the method could work well on Atari games, then we can use it to develop more complicated models like auto submarine\n",
    "# to explore the deep ocean that human could not reach. So in this case we use Seaquest game to see the result.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Data Exploration: Exploring the OpenAI Game System (Gym)\n",
    "\n",
    "**1. Playing Pacman** \n",
    "We first introduce an example of how to use the gym package from OpenAI to design an agent for Pacman Game.\n",
    "In the following cell, we implemented a simple agent, which randomly picks the next action without looking at the screen image (i.e., the *observation*) or the reward.\n",
    "\n",
    "Change the following code for **myAgent** to design a better agent, which takes the observation and reward as the input and picks the best action as the next move.\n",
    "The agent should be able to improve itself after playing more games.\n",
    "\n",
    "***The goals***: Implement an agent using neural networks that can achieve all the following goals:\n",
    "* (a) move the PacMan in all directions.  (5 points)\n",
    "* (b) using neural network to decide what is the best next move. (5 points)\n",
    "* (c) after playing each episode of the game, the agent should be able to improve itself using the experience. (5 points)\n",
    "\n",
    "Action Code:\n",
    "* 1 - UP\n",
    "* 2 - RIGHT\n",
    "* 3 - LEFT\n",
    "* 4 - DOWN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Example code for  processing the screen image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-10 15:18:02,574] Making new env: MsPacman-v0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f95c1c63850>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANEAAAD8CAYAAADpCEEHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f95c3f2ded0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPcAAAD8CAYAAACrSzKQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nB7dZohzcZon6f8Vm2jBNO2T8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f95fbb810d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO8AAAD8CAYAAACfMvOMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ny5SD1yxT/wdRNaeTSNvMNAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f95c1d83b50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "env=gym.make('MsPacman-v0')\n",
    "\n",
    "observation = env.reset()\n",
    "observation, reward, done, info = env.step(1)\n",
    "\n",
    "# plot the current screen\n",
    "plt.figure(1)\n",
    "plt.imshow(np.reshape(observation, (210,160,3) ))\n",
    "\n",
    "#crop the screen\n",
    "x = np.reshape(observation[0:171], (171,160,3) )\n",
    "plt.figure(2)\n",
    "plt.imshow(x)\n",
    "\n",
    "\n",
    "# remove background\n",
    "x = x[:,:,1]\n",
    "x[x==111]=0\n",
    "x[x==28]=1\n",
    "x[x>1]=2\n",
    "# downsample\n",
    "x=x[::2,::2]\n",
    "plt.figure(3)\n",
    "plt.imshow(x,cmap='Greys')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# The Solution: implement a data science solution to the problem you are trying to solve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Briefly describe the idea of your solution to the problem in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Our algorithm is based on the article Deep Reinforcement Learning: Pong from Pixels \n",
    "# In the article, the author applied neural network in Pong game. Computer makes decision of the action \n",
    "# from the historical rewards/punishments (upward or downward).\n",
    "# It’s a binary choice based on the possible score of going up or down\n",
    "# We use deep reinforcement learning to solve the problem and make the pacman smarter. RL is used to make computer automatically play atari games.\n",
    "# To use RL, we can create neural network to let computer learn which action will be the best move in the coming situation based on\n",
    "# the input which is the image of the observation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Write codes to implement the solution in python:\n",
    "***The goals***: Implement an agent using neural networks that can achieve all the following goals:\n",
    "* (a) move the PacMan in all directions.  (5 points)\n",
    "* (b) using neural network to decide what is the best next move. (5 points)\n",
    "* (c) after playing each episode of the game, the agent should be able to improve itself using the experience. (5 points)\n",
    "\n",
    "Action Code:\n",
    "* 1 - UP\n",
    "* 2 - RIGHT\n",
    "* 3 - LEFT\n",
    "* 4 - DOWN\n",
    "\n",
    "* We divide the actions to odd and even numbers. \n",
    "* With odd numbers, we compare the scores of action 1 and 4, and generate the best possible action among this two. \n",
    "* With even numbers, we compare the scores of action 2 and 3, and generate the best possible action.\n",
    "* As a result, pacman can move in all directions!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-15 13:34:57,851] Making new env: MsPacman-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep : 10 average episode reward total was : 169.0\n",
      "ep : 20 average episode reward total was : 160.5\n",
      "ep : 30 average episode reward total was : 165.0\n",
      "ep : 40 average episode reward total was : 162.25\n",
      "ep : 50 average episode reward total was : 167.6\n",
      "ep : 60 average episode reward total was : 166.333333333\n",
      "ep : 70 average episode reward total was : 166.0\n",
      "ep : 80 average episode reward total was : 165.75\n",
      "ep : 90 average episode reward total was : 164.555555556\n",
      "ep : 100 average episode reward total was : 165.3\n",
      "ep : 110 average episode reward total was : 166.454545455\n",
      "ep : 120 average episode reward total was : 165.75\n",
      "ep : 130 average episode reward total was : 163.923076923\n",
      "ep : 140 average episode reward total was : 164.5\n",
      "ep : 150 average episode reward total was : 164.666666667\n",
      "ep : 160 average episode reward total was : 165.125\n",
      "ep : 170 average episode reward total was : 166.176470588\n",
      "ep : 180 average episode reward total was : 165.388888889\n",
      "ep : 190 average episode reward total was : 166.263157895\n",
      "ep : 200 average episode reward total was : 166.85\n",
      "ep : 210 average episode reward total was : 167.476190476\n",
      "ep : 220 average episode reward total was : 168.409090909\n",
      "ep : 230 average episode reward total was : 168.739130435\n",
      "ep : 240 average episode reward total was : 169.75\n",
      "ep : 250 average episode reward total was : 171.04\n",
      "ep : 260 average episode reward total was : 171.884615385\n",
      "ep : 270 average episode reward total was : 171.666666667\n",
      "ep : 280 average episode reward total was : 173.428571429\n",
      "ep : 290 average episode reward total was : 172.655172414\n",
      "ep : 300 average episode reward total was : 171.633333333\n",
      "ep : 310 average episode reward total was : 173.709677419\n",
      "ep : 320 average episode reward total was : 174.625\n",
      "ep : 330 average episode reward total was : 174.272727273\n",
      "ep : 340 average episode reward total was : 175.823529412\n",
      "ep : 350 average episode reward total was : 175.942857143\n",
      "ep : 360 average episode reward total was : 176.222222222\n",
      "ep : 370 average episode reward total was : 176.567567568\n",
      "ep : 380 average episode reward total was : 177.052631579\n",
      "ep : 390 average episode reward total was : 176.179487179\n",
      "ep : 400 average episode reward total was : 176.05\n",
      "ep : 410 average episode reward total was : 176.707317073\n",
      "ep : 420 average episode reward total was : 176.547619048\n",
      "ep : 430 average episode reward total was : 176.395348837\n",
      "ep : 440 average episode reward total was : 176.25\n",
      "ep : 450 average episode reward total was : 176.911111111\n",
      "ep : 460 average episode reward total was : 178.239130435\n",
      "ep : 470 average episode reward total was : 178.042553191\n",
      "ep : 480 average episode reward total was : 178.729166667\n",
      "ep : 490 average episode reward total was : 178.897959184\n",
      "ep : 500 average episode reward total was : 179.38\n",
      "ep : 510 average episode reward total was : 179.156862745\n",
      "ep : 520 average episode reward total was : 179.826923077\n",
      "ep : 530 average episode reward total was : 179.811320755\n",
      "ep : 540 average episode reward total was : 179.462962963\n",
      "ep : 550 average episode reward total was : 179.145454545\n",
      "ep : 560 average episode reward total was : 179.803571429\n",
      "ep : 570 average episode reward total was : 179.596491228\n",
      "ep : 580 average episode reward total was : 179.517241379\n",
      "ep : 590 average episode reward total was : 179.966101695\n",
      "ep : 600 average episode reward total was : 179.583333333\n",
      "ep : 610 average episode reward total was : 179.606557377\n",
      "ep : 620 average episode reward total was : 179.467741935\n",
      "ep : 630 average episode reward total was : 179.476190476\n",
      "ep : 640 average episode reward total was : 179.046875\n",
      "ep : 650 average episode reward total was : 180.092307692\n",
      "ep : 660 average episode reward total was : 179.909090909\n",
      "ep : 670 average episode reward total was : 179.641791045\n",
      "ep : 680 average episode reward total was : 179.794117647\n",
      "ep : 690 average episode reward total was : 179.898550725\n",
      "ep : 700 average episode reward total was : 179.742857143\n",
      "ep : 710 average episode reward total was : 179.492957746\n",
      "ep : 720 average episode reward total was : 179.638888889\n",
      "ep : 730 average episode reward total was : 180.767123288\n",
      "ep : 740 average episode reward total was : 181.013513514\n",
      "ep : 750 average episode reward total was : 180.506666667\n",
      "ep : 760 average episode reward total was : 180.802631579\n",
      "ep : 770 average episode reward total was : 180.688311688\n",
      "ep : 780 average episode reward total was : 180.512820513\n",
      "ep : 790 average episode reward total was : 180.746835443\n",
      "ep : 800 average episode reward total was : 180.6875\n",
      "ep : 810 average episode reward total was : 181.382716049\n",
      "ep : 820 average episode reward total was : 181.585365854\n",
      "ep : 830 average episode reward total was : 181.385542169\n",
      "ep : 840 average episode reward total was : 181.476190476\n",
      "ep : 850 average episode reward total was : 182.211764706\n",
      "ep : 860 average episode reward total was : 182.802325581\n",
      "ep : 870 average episode reward total was : 183.310344828\n",
      "ep : 880 average episode reward total was : 183.329545455\n",
      "ep : 890 average episode reward total was : 183.224719101\n",
      "ep : 900 average episode reward total was : 182.9\n",
      "ep : 910 average episode reward total was : 183.21978022\n",
      "ep : 920 average episode reward total was : 183.358695652\n",
      "ep : 930 average episode reward total was : 183.623655914\n",
      "ep : 940 average episode reward total was : 183.563829787\n",
      "ep : 950 average episode reward total was : 183.389473684\n",
      "ep : 960 average episode reward total was : 183.604166667\n",
      "ep : 970 average episode reward total was : 183.505154639\n",
      "ep : 980 average episode reward total was : 184.193877551\n",
      "ep : 990 average episode reward total was : 184.343434343\n",
      "ep : 1000 average episode reward total was : 184.14\n",
      "ep : 1010 average episode reward total was : 184.831683168\n",
      "ep : 1020 average episode reward total was : 184.901960784\n",
      "ep : 1030 average episode reward total was : 185.067961165\n",
      "ep : 1040 average episode reward total was : 185.153846154\n",
      "ep : 1050 average episode reward total was : 184.866666667\n",
      "ep : 1060 average episode reward total was : 185.113207547\n",
      "ep : 1070 average episode reward total was : 185.317757009\n",
      "ep : 1080 average episode reward total was : 185.185185185\n",
      "ep : 1090 average episode reward total was : 185.275229358\n",
      "ep : 1100 average episode reward total was : 185.218181818\n",
      "ep : 1110 average episode reward total was : 185.765765766\n",
      "ep : 1120 average episode reward total was : 186.241071429\n",
      "ep : 1130 average episode reward total was : 186.203539823\n",
      "ep : 1140 average episode reward total was : 186.719298246\n",
      "ep : 1150 average episode reward total was : 186.904347826\n",
      "ep : 1160 average episode reward total was : 187.629310345\n",
      "ep : 1170 average episode reward total was : 188.0\n",
      "ep : 1180 average episode reward total was : 189.025423729\n",
      "ep : 1190 average episode reward total was : 189.294117647\n",
      "ep : 1200 average episode reward total was : 189.683333333\n",
      "ep : 1210 average episode reward total was : 191.033057851\n",
      "ep : 1220 average episode reward total was : 191.172131148\n",
      "ep : 1230 average episode reward total was : 191.56097561\n",
      "ep : 1240 average episode reward total was : 192.637096774\n",
      "ep : 1250 average episode reward total was : 192.64\n",
      "ep : 1260 average episode reward total was : 192.404761905\n",
      "ep : 1270 average episode reward total was : 193.181102362\n",
      "ep : 1280 average episode reward total was : 192.8828125\n",
      "ep : 1290 average episode reward total was : 193.224806202\n",
      "ep : 1300 average episode reward total was : 193.284615385\n",
      "ep : 1310 average episode reward total was : 193.320610687\n",
      "ep : 1320 average episode reward total was : 193.568181818\n",
      "ep : 1330 average episode reward total was : 193.563909774\n",
      "ep : 1340 average episode reward total was : 193.5\n",
      "ep : 1350 average episode reward total was : 193.674074074\n",
      "ep : 1360 average episode reward total was : 195.080882353\n",
      "ep : 1370 average episode reward total was : 195.094890511\n",
      "ep : 1380 average episode reward total was : 195.202898551\n",
      "ep : 1390 average episode reward total was : 195.136690647\n",
      "ep : 1400 average episode reward total was : 194.821428571\n",
      "ep : 1410 average episode reward total was : 194.780141844\n",
      "ep : 1420 average episode reward total was : 194.521126761\n",
      "ep : 1430 average episode reward total was : 194.587412587\n",
      "ep : 1440 average episode reward total was : 194.875\n",
      "ep : 1450 average episode reward total was : 194.751724138\n",
      "ep : 1460 average episode reward total was : 194.794520548\n",
      "ep : 1470 average episode reward total was : 195.394557823\n",
      "ep : 1480 average episode reward total was : 195.182432432\n",
      "ep : 1490 average episode reward total was : 195.496644295\n",
      "ep : 1500 average episode reward total was : 195.573333333\n",
      "ep : 1510 average episode reward total was : 195.331125828\n",
      "ep : 1520 average episode reward total was : 195.190789474\n",
      "ep : 1530 average episode reward total was : 195.901960784\n",
      "ep : 1540 average episode reward total was : 195.909090909\n",
      "ep : 1550 average episode reward total was : 196.15483871\n",
      "ep : 1560 average episode reward total was : 196.442307692\n",
      "ep : 1570 average episode reward total was : 196.452229299\n",
      "ep : 1580 average episode reward total was : 196.569620253\n",
      "ep : 1590 average episode reward total was : 196.616352201\n",
      "ep : 1600 average episode reward total was : 196.7375\n",
      "ep : 1610 average episode reward total was : 196.770186335\n",
      "ep : 1620 average episode reward total was : 196.833333333\n",
      "ep : 1630 average episode reward total was : 197.257668712\n",
      "ep : 1640 average episode reward total was : 197.219512195\n",
      "ep : 1650 average episode reward total was : 197.309090909\n",
      "ep : 1660 average episode reward total was : 198.210843373\n",
      "ep : 1670 average episode reward total was : 198.419161677\n",
      "ep : 1680 average episode reward total was : 198.482142857\n",
      "ep : 1690 average episode reward total was : 198.50887574\n",
      "ep : 1700 average episode reward total was : 198.888235294\n",
      "ep : 1710 average episode reward total was : 199.140350877\n",
      "ep : 1720 average episode reward total was : 199.284883721\n",
      "ep : 1730 average episode reward total was : 200.722543353\n",
      "ep : 1740 average episode reward total was : 201.373563218\n",
      "ep : 1750 average episode reward total was : 201.554285714\n",
      "ep : 1760 average episode reward total was : 201.6875\n",
      "ep : 1770 average episode reward total was : 201.644067797\n",
      "ep : 1780 average episode reward total was : 201.662921348\n",
      "ep : 1790 average episode reward total was : 201.569832402\n",
      "ep : 1800 average episode reward total was : 201.605555556\n",
      "ep : 1810 average episode reward total was : 201.756906077\n",
      "ep : 1820 average episode reward total was : 202.78021978\n",
      "ep : 1830 average episode reward total was : 202.530054645\n",
      "ep : 1840 average episode reward total was : 202.483695652\n",
      "ep : 1850 average episode reward total was : 202.524324324\n",
      "ep : 1860 average episode reward total was : 203.247311828\n",
      "ep : 1870 average episode reward total was : 203.187165775\n",
      "ep : 1880 average episode reward total was : 203.425531915\n",
      "ep : 1890 average episode reward total was : 203.417989418\n",
      "ep : 1900 average episode reward total was : 203.763157895\n",
      "ep : 1910 average episode reward total was : 203.691099476\n",
      "ep : 1920 average episode reward total was : 203.703125\n",
      "ep : 1930 average episode reward total was : 204.010362694\n",
      "ep : 1940 average episode reward total was : 204.432989691\n",
      "ep : 1950 average episode reward total was : 204.61025641\n",
      "ep : 1960 average episode reward total was : 204.862244898\n",
      "ep : 1970 average episode reward total was : 204.83248731\n",
      "ep : 1980 average episode reward total was : 205.848484848\n",
      "ep : 1990 average episode reward total was : 206.085427136\n",
      "ep : 2000 average episode reward total was : 206.3\n",
      "ep : 2010 average episode reward total was : 206.462686567\n",
      "ep : 2020 average episode reward total was : 206.534653465\n",
      "ep : 2030 average episode reward total was : 206.566502463\n",
      "ep : 2040 average episode reward total was : 206.607843137\n",
      "ep : 2050 average episode reward total was : 206.931707317\n",
      "ep : 2060 average episode reward total was : 206.873786408\n",
      "ep : 2070 average episode reward total was : 208.52173913\n",
      "ep : 2080 average episode reward total was : 208.990384615\n",
      "ep : 2090 average episode reward total was : 210.306220096\n",
      "ep : 2100 average episode reward total was : 210.480952381\n",
      "ep : 2110 average episode reward total was : 210.748815166\n",
      "ep : 2120 average episode reward total was : 211.028301887\n",
      "ep : 2130 average episode reward total was : 211.122065728\n",
      "ep : 2140 average episode reward total was : 211.028037383\n",
      "ep : 2150 average episode reward total was : 211.427906977\n",
      "ep : 2160 average episode reward total was : 211.50462963\n",
      "ep : 2170 average episode reward total was : 211.493087558\n",
      "ep : 2180 average episode reward total was : 211.97706422\n",
      "ep : 2190 average episode reward total was : 212.04109589\n",
      "ep : 2200 average episode reward total was : 212.404545455\n",
      "ep : 2210 average episode reward total was : 212.574660633\n",
      "ep : 2220 average episode reward total was : 212.756756757\n",
      "ep : 2230 average episode reward total was : 212.820627803\n",
      "ep : 2240 average episode reward total was : 213.09375\n",
      "ep : 2250 average episode reward total was : 213.835555556\n",
      "ep : 2260 average episode reward total was : 213.756637168\n",
      "ep : 2270 average episode reward total was : 213.797356828\n",
      "ep : 2280 average episode reward total was : 213.728070175\n",
      "ep : 2290 average episode reward total was : 213.698689956\n",
      "ep : 2300 average episode reward total was : 214.060869565\n",
      "ep : 2310 average episode reward total was : 214.571428571\n",
      "ep : 2320 average episode reward total was : 215.021551724\n",
      "ep : 2330 average episode reward total was : 215.124463519\n",
      "ep : 2340 average episode reward total was : 215.320512821\n",
      "ep : 2350 average episode reward total was : 215.659574468\n",
      "ep : 2360 average episode reward total was : 215.711864407\n",
      "ep : 2370 average episode reward total was : 215.729957806\n",
      "ep : 2380 average episode reward total was : 215.621848739\n",
      "ep : 2390 average episode reward total was : 215.673640167\n",
      "ep : 2400 average episode reward total was : 216.091666667\n",
      "ep : 2410 average episode reward total was : 216.211618257\n",
      "ep : 2420 average episode reward total was : 216.425619835\n",
      "ep : 2430 average episode reward total was : 216.70781893\n",
      "ep : 2440 average episode reward total was : 217.717213115\n",
      "ep : 2450 average episode reward total was : 217.820408163\n",
      "ep : 2460 average episode reward total was : 218.048780488\n",
      "ep : 2470 average episode reward total was : 217.975708502\n",
      "ep : 2480 average episode reward total was : 218.016129032\n",
      "ep : 2490 average episode reward total was : 218.0562249\n",
      "ep : 2500 average episode reward total was : 218.32\n",
      "ep : 2510 average episode reward total was : 218.358565737\n",
      "ep : 2520 average episode reward total was : 218.996031746\n",
      "ep : 2530 average episode reward total was : 218.822134387\n",
      "ep : 2540 average episode reward total was : 219.326771654\n",
      "ep : 2550 average episode reward total was : 219.62745098\n",
      "ep : 2560 average episode reward total was : 219.81640625\n",
      "ep : 2570 average episode reward total was : 219.828793774\n",
      "ep : 2580 average episode reward total was : 219.798449612\n",
      "ep : 2590 average episode reward total was : 220.467181467\n",
      "ep : 2600 average episode reward total was : 220.6\n",
      "ep : 2610 average episode reward total was : 220.651340996\n",
      "ep : 2620 average episode reward total was : 220.736641221\n",
      "ep : 2630 average episode reward total was : 220.787072243\n",
      "ep : 2640 average episode reward total was : 221.011363636\n",
      "ep : 2650 average episode reward total was : 221.294339623\n",
      "ep : 2660 average episode reward total was : 221.507518797\n",
      "ep : 2670 average episode reward total was : 221.913857678\n",
      "ep : 2680 average episode reward total was : 222.018656716\n",
      "ep : 2690 average episode reward total was : 222.104089219\n",
      "ep : 2700 average episode reward total was : 222.533333333\n",
      "ep : 2710 average episode reward total was : 222.457564576\n",
      "ep : 2720 average episode reward total was : 223.0\n",
      "ep : 2730 average episode reward total was : 223.212454212\n",
      "ep : 2740 average episode reward total was : 223.109489051\n",
      "ep : 2750 average episode reward total was : 223.214545455\n",
      "ep : 2760 average episode reward total was : 223.315217391\n",
      "ep : 2770 average episode reward total was : 223.646209386\n",
      "ep : 2780 average episode reward total was : 223.633093525\n",
      "ep : 2790 average episode reward total was : 223.698924731\n",
      "ep : 2800 average episode reward total was : 223.928571429\n",
      "ep : 2810 average episode reward total was : 224.06405694\n",
      "ep : 2820 average episode reward total was : 224.060283688\n",
      "ep : 2830 average episode reward total was : 224.424028269\n",
      "ep : 2840 average episode reward total was : 225.112676056\n",
      "ep : 2850 average episode reward total was : 225.063157895\n",
      "ep : 2860 average episode reward total was : 225.29020979\n",
      "ep : 2870 average episode reward total was : 225.236933798\n",
      "ep : 2880 average episode reward total was : 225.0625\n",
      "ep : 2890 average episode reward total was : 225.311418685\n",
      "ep : 2900 average episode reward total was : 225.5\n",
      "ep : 2910 average episode reward total was : 225.855670103\n",
      "ep : 2920 average episode reward total was : 225.828767123\n",
      "ep : 2930 average episode reward total was : 226.068259386\n",
      "ep : 2940 average episode reward total was : 226.149659864\n",
      "ep : 2950 average episode reward total was : 226.33220339\n",
      "ep : 2960 average episode reward total was : 226.540540541\n",
      "ep : 2970 average episode reward total was : 227.188552189\n",
      "ep : 2980 average episode reward total was : 227.684563758\n",
      "ep : 2990 average episode reward total was : 227.829431438\n",
      "ep : 3000 average episode reward total was : 227.876666667\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niEhWShAiIpKVEoSIiGSlBCEiIln9P5yFXE79OWteAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f98c56c1990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\" Trains an agent with (stochastic) Policy Gradients on Pacman. Uses OpenAI Gym. \"\"\"\n",
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# hyperparameters\n",
    "H = 200 # number of hidden layer neurons\n",
    "batch_size = 10 # every how many episodes to do a param update?\n",
    "learning_rate = 1e-4\n",
    "gamma = 0.99 # discount factor for reward\n",
    "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
    "resume = False # resume from previous checkpoint?\n",
    "render = False\n",
    "\n",
    "# model initialization\n",
    "D = 80 * 80 # input dimensionality: 80x80 grid\n",
    "if resume:\n",
    "    model = pickle.load(open('save.p', 'rb'))\n",
    "else:\n",
    "    model = {}\n",
    "    model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n",
    "    model['W2'] = np.random.randn(H) / np.sqrt(H)\n",
    "  \n",
    "grad_buffer = { k : np.zeros_like(v) for k,v in model.iteritems() } # update buffers that add up gradients over a batch\n",
    "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.iteritems() } # rmsprop memory\n",
    "\n",
    "def sigmoid(x): \n",
    "    return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
    "\n",
    "def prepro(I):\n",
    "    \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "    I = I[35:195] # crop\n",
    "    I = I[::2,::2,0] # downsample by factor of 2\n",
    "    I[I == 144] = 0 # erase background (background type 1)\n",
    "    I[I == 109] = 0 # erase background (background type 2)\n",
    "    I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "    return I.astype(np.float).ravel()\n",
    "\n",
    "def discount_rewards(r):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(xrange(0, r.size)):\n",
    "        if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (Pacman specific!)\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r\n",
    "\n",
    "def policy_forward(x):\n",
    "    h = np.dot(model['W1'], x)\n",
    "    h[h<0] = 0 # ReLU nonlinearity\n",
    "    logp = np.dot(model['W2'], h)\n",
    "    p = sigmoid(logp)\n",
    "    return p, h # return probability of taking action 2, and hidden state\n",
    "\n",
    "def policy_backward(eph, epdlogp):\n",
    "    \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
    "    dW2 = np.dot(eph.T, epdlogp).ravel()\n",
    "    dh = np.outer(epdlogp, model['W2'])\n",
    "    dh[eph <= 0] = 0 # backpro prelu\n",
    "    dW1 = np.dot(dh.T, epx)\n",
    "    return {'W1':dW1, 'W2':dW2}\n",
    "\n",
    "env = gym.make(\"MsPacman-v0\")\n",
    "observation = env.reset()\n",
    "prev_x = None # used in computing the difference frame\n",
    "xs,hs,dlogps,drs = [],[],[],[]\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 0\n",
    "n = 0\n",
    "history_sum = 0\n",
    "rewards=[]\n",
    "\n",
    "while episode_number < 3000:\n",
    "    if episode_number == 2999:\n",
    "        render = True\n",
    "\n",
    "    if n%2==0:\n",
    "        action_1 = 2\n",
    "        action_2 = 3\n",
    "    else:\n",
    "        action_1 = 1\n",
    "        action_2 = 4\n",
    "    n+=1\n",
    "    \n",
    "    if render: env.render()\n",
    "\n",
    "    # preprocess the observation, set input to network to be difference image\n",
    "    cur_x = prepro(observation)\n",
    "    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
    "    prev_x = cur_x\n",
    "\n",
    "    # forward the policy network and sample an action from the returned probability\n",
    "    aprob, h = policy_forward(x)\n",
    "    action = action_1 if np.random.uniform() < aprob else action_2 # roll the dice!\n",
    "\n",
    "    # record various intermediates (needed later for backprop)\n",
    "    xs.append(x) # observation\n",
    "    hs.append(h) # hidden state\n",
    "    y = 1 if action == action_1 else 0 # a \"fake label\"\n",
    "    dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n",
    "\n",
    "    # step the environment and get new measurements\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    reward_sum += reward\n",
    "    \n",
    "    drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
    "\n",
    "    if done: # an episode finished\n",
    "        episode_number += 1\n",
    "        history_sum += reward_sum\n",
    "        ave = history_sum/episode_number\n",
    "        rewards.append(ave)\n",
    "\n",
    "    # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
    "        epx = np.vstack(xs)\n",
    "        eph = np.vstack(hs)\n",
    "        epdlogp = np.vstack(dlogps)\n",
    "        epr = np.vstack(drs)\n",
    "        xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n",
    "\n",
    "    # compute the discounted reward backwards through time\n",
    "        discounted_epr = discount_rewards(epr)\n",
    "    # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
    "        discounted_epr -= np.mean(discounted_epr)\n",
    "        discounted_epr /= np.std(discounted_epr)\n",
    "\n",
    "        epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
    "        grad = policy_backward(eph, epdlogp)\n",
    "        for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n",
    "\n",
    "    # perform rmsprop parameter update every batch_size episodes\n",
    "        if episode_number % batch_size == 0:\n",
    "            for k,v in model.iteritems():\n",
    "                g = grad_buffer[k] # gradient\n",
    "                rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
    "                model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
    "                grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
    "\n",
    "    # boring book-keeping\n",
    "        #running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "        \n",
    "        if episode_number%10 == 0:\n",
    "            print 'ep :', episode_number, 'average episode reward total was :', ave\n",
    "        #running mean: %f' % (reward_sum, running_reward)\n",
    "        if episode_number % 100 == 0: pickle.dump(model, open('save.p', 'wb'))\n",
    "        reward_sum = 0\n",
    "        observation = env.reset() # reset env\n",
    "        prev_x = None\n",
    "\n",
    "    #if reward != 0: # Pacman has either +1 or -1 reward exactly when game ends.\n",
    "    #print ('ep %d: game finished, reward: %f' % (episode_number, reward)) + ('' if reward == -1 else ' !!!!!!!!')\n",
    "\n",
    "x = range(0,3000)\n",
    "plt.title(\"Rewards vs episodes\") \n",
    "plt.plot(x, rewards, color=\"red\")\n",
    "plt.xlabel(\"# episodes\")\n",
    "plt.ylabel(\"Rewards\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## The following code is for Seaquest game\n",
    "\n",
    "\"\"\" Trains an agent with (stochastic) Policy Gradients on Seaquest. Uses OpenAI Gym. \"\"\"\n",
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# hyperparameters\n",
    "H = 200 # number of hidden layer neurons\n",
    "batch_size = 10 # every how many episodes to do a param update?\n",
    "learning_rate = 1e-4\n",
    "gamma = 0.99 # discount factor for reward\n",
    "decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n",
    "resume = False # resume from previous checkpoint?\n",
    "render = False\n",
    "\n",
    "# model initialization\n",
    "D = 80 * 80 # input dimensionality: 80x80 grid\n",
    "if resume:\n",
    "    model = pickle.load(open('save.p', 'rb'))\n",
    "else:\n",
    "    model = {}\n",
    "    model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n",
    "    model['W2'] = np.random.randn(H) / np.sqrt(H)\n",
    "  \n",
    "grad_buffer = { k : np.zeros_like(v) for k,v in model.iteritems() } # update buffers that add up gradients over a batch\n",
    "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.iteritems() } # rmsprop memory\n",
    "\n",
    "def sigmoid(x): \n",
    "    return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n",
    "\n",
    "def prepro(I):\n",
    "    \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "    I = I[35:195] # crop\n",
    "    I = I[::2,::2,0] # downsample by factor of 2\n",
    "    I[I == 144] = 0 # erase background (background type 1)\n",
    "    I[I == 109] = 0 # erase background (background type 2)\n",
    "    I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "    return I.astype(np.float).ravel()\n",
    "\n",
    "def discount_rewards(r):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(xrange(0, r.size)):\n",
    "        if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (Seaquest specific!)\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r\n",
    "\n",
    "def policy_forward(x):\n",
    "    h = np.dot(model['W1'], x)\n",
    "    h[h<0] = 0 # ReLU nonlinearity\n",
    "    logp = np.dot(model['W2'], h)\n",
    "    p = sigmoid(logp)\n",
    "    return p, h # return probability of taking action 2, and hidden state\n",
    "\n",
    "def policy_backward(eph, epdlogp):\n",
    "    \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n",
    "    dW2 = np.dot(eph.T, epdlogp).ravel()\n",
    "    dh = np.outer(epdlogp, model['W2'])\n",
    "    dh[eph <= 0] = 0 # backpro prelu\n",
    "    dW1 = np.dot(dh.T, epx)\n",
    "    return {'W1':dW1, 'W2':dW2}\n",
    "\n",
    "env = gym.make(\"Seaquest-v0\")\n",
    "observation = env.reset()\n",
    "prev_x = None # used in computing the difference frame\n",
    "xs,hs,dlogps,drs = [],[],[],[]\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 0\n",
    "n = 0\n",
    "history_sum = 0\n",
    "rewards=[]\n",
    "\n",
    "while episode_number < 1000:\n",
    "    if episode_number == 999:\n",
    "        render = True\n",
    "\n",
    "    if n%4 == 0:\n",
    "        action_1 = 2\n",
    "        action_2 = 5\n",
    "    if n%4 == 1:\n",
    "        action_1 = 1\n",
    "        action_2 = 1\n",
    "    if n%4 == 2:\n",
    "        action_1 = 8\n",
    "        action_2 = 9\n",
    "    if n%4 == 3:\n",
    "        action_1 = 1\n",
    "        action_2 = 1\n",
    "    n+=1\n",
    "    \n",
    "    if render: env.render()\n",
    "\n",
    "  # preprocess the observation, set input to network to be difference image\n",
    "    cur_x = prepro(observation)\n",
    "    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
    "    prev_x = cur_x\n",
    "\n",
    "  # forward the policy network and sample an action from the returned probability\n",
    "    aprob, h = policy_forward(x)\n",
    "    action = action_1 if np.random.uniform() < aprob else action_2 # roll the dice!\n",
    "\n",
    "  # record various intermediates (needed later for backprop)\n",
    "    xs.append(x) # observation\n",
    "    hs.append(h) # hidden state\n",
    "    y = 1 if action == action_1 else 0 # a \"fake label\"\n",
    "    dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n",
    "\n",
    "  # step the environment and get new measurements\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    reward_sum += reward\n",
    "    \n",
    "    drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
    "\n",
    "    if done: # an episode finished\n",
    "        episode_number += 1\n",
    "        history_sum += reward_sum\n",
    "        ave = history_sum/episode_number\n",
    "        rewards.append(ave)\n",
    "\n",
    "    # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
    "        epx = np.vstack(xs)\n",
    "        eph = np.vstack(hs)\n",
    "        epdlogp = np.vstack(dlogps)\n",
    "        epr = np.vstack(drs)\n",
    "        xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n",
    "\n",
    "    # compute the discounted reward backwards through time\n",
    "        discounted_epr = discount_rewards(epr)\n",
    "    # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n",
    "        discounted_epr -= np.mean(discounted_epr)\n",
    "        discounted_epr /= np.std(discounted_epr)\n",
    "\n",
    "        epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n",
    "        grad = policy_backward(eph, epdlogp)\n",
    "        for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n",
    "\n",
    "    # perform rmsprop parameter update every batch_size episodes\n",
    "        if episode_number % batch_size == 0:\n",
    "            for k,v in model.iteritems():\n",
    "                g = grad_buffer[k] # gradient\n",
    "                rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n",
    "                model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n",
    "                grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n",
    "\n",
    "    # boring book-keeping\n",
    "        #running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "\n",
    "        if episode_number%5 == 0:\n",
    "            print 'ep :', episode_number, 'average episode reward total was :', ave\n",
    "        #running mean: %f' % (reward_sum, running_reward)\n",
    "        if episode_number % 100 == 0: pickle.dump(model, open('save.p', 'wb'))\n",
    "        reward_sum = 0\n",
    "        observation = env.reset() # reset env\n",
    "        prev_x = None\n",
    "\n",
    "    #if reward != 0: # Seaquest has either +1 or -1 reward exactly when game ends.\n",
    "    #print ('ep %d: game finished, reward: %f' % (episode_number, reward)) + ('' if reward == -1 else ' !!!!!!!!')\n",
    "\n",
    "x = range(0,1000)\n",
    "plt.title(\"Rewards vs episodes\") \n",
    "plt.plot(x, rewards, color=\"red\")\n",
    "plt.xlabel(\"# episodes\")\n",
    "plt.ylabel(\"Rewards\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Results: summarize and visualize the results discovered from the analysis\n",
    "\n",
    "Please use figures, tables, or videos to communicate the results with the audience.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# As we can see above, the average number of rewards is increasing all the time, so our algorithm work well on pacman.\n",
    "# We don't simulate too many episodes due to the time limatation, if we can have enough time, the score will increase a lot\n",
    "# compared with the very firt step as you can see in our video.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "*-----------------\n",
    "# Done\n",
    "\n",
    "All set! \n",
    "\n",
    "** What do you need to submit?**\n",
    "\n",
    "* **Notebook File**: Save this Jupyter notebook, and find the notebook file in your folder (for example, \"filename.ipynb\"). This is the file you need to submit. Please make sure all the plotted tables and figures are in the notebook. If you used \"jupyter notebook --pylab=inline\" to open the notebook, all the figures and tables should have shown up in the notebook.\n",
    "\n",
    "* **PPT Slides**: please prepare PPT slides (for 10 minutes' talk) to present about the case study . Each team present their case studies in class for 10 minutes.\n",
    "\n",
    "Please compress all the files in a zipped file.\n",
    "\n",
    "\n",
    "** How to submit: **\n",
    "\n",
    "        Please submit through Canvas, in the Assignment \"Case Study 3\".\n",
    "        \n",
    "** Note: Each team only needs to submit one submission in Canvas **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "# Peer-Review Grading Template:\n",
    "\n",
    "** Total Points: (100 points) ** Please don't worry about the absolute scores, we will rescale the final grading according to the performance of all teams in the class.\n",
    "\n",
    "Please add an \"**X**\" mark in front of your rating: \n",
    "\n",
    "For example:\n",
    "\n",
    "*2: bad*\n",
    "          \n",
    "**X** *3: good*\n",
    "    \n",
    "*4: perfect*\n",
    "\n",
    "\n",
    "    ---------------------------------\n",
    "    The Problem: \n",
    "    ---------------------------------\n",
    "    \n",
    "    1. (10 points) how well did the team describe the problem they are trying to solve using the data? \n",
    "       0: not clear\n",
    "       2: I can barely understand the problem\n",
    "       4: okay, can be improved\n",
    "       6: good, but can be improved\n",
    "       8: very good\n",
    "       10: crystal clear\n",
    "    \n",
    "    2. (10 points) do you think the problem is important or has a potential impact?\n",
    "        0: not important at all\n",
    "        2: not sure if it is important\n",
    "        4: seems important, but not clear\n",
    "        6: interesting problem\n",
    "        8: an important problem, which I want to know the answer myself\n",
    "       10: very important, I would be happy invest money on a project like this.\n",
    "    \n",
    "    ----------------------------------\n",
    "    Data Collection and Processing:\n",
    "    ----------------------------------\n",
    "    \n",
    "    3. (5 points) Do you think the data collected/processed are relevant and sufficient for solving the above problem? \n",
    "       0: not clear\n",
    "       1: I can barely understand what data they are trying to collect/process\n",
    "       2: I can barely understand why the data is relevant to the problem\n",
    "       3: the data are relevant to the problem, but better data can be collected\n",
    "       4: the data collected are relevant and at a proper scale\n",
    "       5: the data are properly collected and they are sufficient\n",
    "\n",
    "    -----------------------------------\n",
    "    Data Exploration:\n",
    "    -----------------------------------\n",
    "    4. How well did the team solve the exploration task (15 points):\n",
    "       0: missing answer\n",
    "       5: okay, but with major problems\n",
    "      10: good, but with minor problems\n",
    "      15: perfect\n",
    "    \n",
    "    \n",
    "    -----------------------------------\n",
    "    The Solution\n",
    "    -----------------------------------\n",
    "    5.  how well did the team describe the solution they used to solve the problem? (10 points)\n",
    "       0: not clear\n",
    "       2: I can barely understand\n",
    "       4: okay, can be improved\n",
    "       6: good, but can be improved\n",
    "       8: very good\n",
    "       10: crystal clear\n",
    "       \n",
    "    6. how well is the solution in solving the problem? (10 points)\n",
    "       0: not relevant\n",
    "       2: barely relevant to the problem\n",
    "       4: okay solution, but there is an easier solution.\n",
    "       6: good, but can be improved\n",
    "       8: very good, but solution is simple/old\n",
    "       10: innovative and technically sound\n",
    "       \n",
    "    7. how well did the team implement the solution in python? (10 points)\n",
    "       0: the code is not relevant to the solution proposed\n",
    "       2: the code is barely understandable, but not relevant\n",
    "       4: okay, the code is clear but incorrect\n",
    "       6: good, the code is correct, but with major errors\n",
    "       8: very good, the code is correct, but with minor errors\n",
    "      10: perfect \n",
    "   \n",
    "    -----------------------------------\n",
    "    The Results\n",
    "    -----------------------------------\n",
    "     8.  How well did the team present the results they found in the data? (10 points)\n",
    "       0: not clear\n",
    "       2: I can barely understand\n",
    "       4: okay, can be improved\n",
    "       6: good, but can be improved\n",
    "       8: very good\n",
    "      10: crystal clear\n",
    "       \n",
    "     9.  How do you think of the results they found in the data?  (5 points)\n",
    "       0: not clear\n",
    "       1: likely to be wrong\n",
    "       2: okay, maybe wrong\n",
    "       3: good, but can be improved\n",
    "       4: make sense, but not interesting\n",
    "       5: make sense and very interesting\n",
    "     \n",
    "    -----------------------------------\n",
    "    The Presentation\n",
    "    -----------------------------------\n",
    "    10. How all the different parts (data, problem, solution, result) fit together as a coherent story?  \n",
    "       0: they are irrelevant\n",
    "       1: I can barely understand how they are related to each other\n",
    "       2: okay, the problem is good, but the solution doesn't match well, or the problem is not solvable.\n",
    "       3: good, but the results don't make much sense in the context\n",
    "       4: very good fit, but not exciting (the storyline can be improved/polished)\n",
    "       5: a perfect story\n",
    "      \n",
    "    11. Did the presenter make good use of the 10 minutes for presentation?  \n",
    "       0: the team didn't present\n",
    "       1: bad, barely finished a small part of the talk\n",
    "       2: okay, barely finished most parts of the talk.\n",
    "       3: good, finished all parts of the talk, but some part is rushed\n",
    "       4: very good, but the allocation of time on different parts can be improved.\n",
    "       5: perfect timing and good use of time      \n",
    "\n",
    "    12. How well do you think of the presentation (overall quality)?  \n",
    "       0: the team didn't present\n",
    "       1: bad\n",
    "       2: okay\n",
    "       3: good\n",
    "       4: very good\n",
    "       5: perfect\n",
    "\n",
    "\n",
    "    -----------------------------------\n",
    "    Overall: \n",
    "    -----------------------------------\n",
    "    13. How many points out of the 100 do you give to this project in total?  Please don't worry about the absolute scores, we will rescale the final grading according to the performance of all teams in the class.\n",
    "    Total score:\n",
    "    \n",
    "    14. What are the strengths of this project? Briefly, list up to 3 strengths.\n",
    "       1: \n",
    "       2:\n",
    "       3:\n",
    "    \n",
    "    15. What are the weaknesses of this project? Briefly, list up to 3 weaknesses.\n",
    "       1:\n",
    "       2:\n",
    "       3:\n",
    "    \n",
    "    16. Detailed comments and suggestions. What suggestions do you have for this project to improve its quality further.\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    ---------------------------------\n",
    "    Your Vote: \n",
    "    ---------------------------------\n",
    "    1. [Overall Quality] Between the two submissions that you are reviewing, which team would you vote for a better score?  (5 bonus points)\n",
    "        0: I vote the other team is better than this team\n",
    "        5: I vote this team is better than the other team \n",
    "        \n",
    "    2. [Presentation] Among all the teams in the presentation, which team do you think deserves the best presentation award for this case study?  \n",
    "        1: Team 1\n",
    "        2: Team 2\n",
    "        3: Team 3\n",
    "        4: Team 4\n",
    "        5: Team 5\n",
    "        6: Team 6\n",
    "        7: Team 7\n",
    "        8: Team 8\n",
    "        9: Team 9\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
